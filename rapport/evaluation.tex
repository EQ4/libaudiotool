\chapter{Evaluation}
\section{Main evaluation idea}
To evaluate the robustness of the watermark techniques that we implemented, we decided to compare the strength of the techniques against different attacks with the \ac{LSB} technique. Since the \ac{LSB} technique was the most easy one to implement, this has been decided from the begining. Hence, we compute the score of the other techniques with the score of the \ac{LSB} algorithm as a reference. \\

The scores are computed by calculating the number of false bits in a watermarked data after altering it with a degradation. To do so, we implemented different benchmark algorithms that degrade the signal. \\
\section{Existing work}
Many evaluation tools for watermarking techniques exist and they have inspired our work. 
The first of them, which is the one that inspired most of our evaluations, is the \textbf{StirMark Benchmark}. It's a generic tool for simple robustness testing of image watermarking algorithms. 

Even if we focused on the audio watermark, it was helpful to learn about it. It introduces several geometric distortions to de-synchronise watermarking algorithms such as:
\begin{itemize}
 \item Add a noise to the signal
 \item Amplify  the signal
 \item High pass / Low pass filters
 \item Delay...
\end{itemize}

An audio version of the \brand{StirMark Benchmark}, called \brand{StirMark Benchmark for Audio} used to exist, however, it was removed from the internet and is now only accessible using \brand{Google Cache}. This was the main basis for our work.

Another existing evaluation tool is the \textbf{Watermark Evaluation TestBed (WET)}. However, we did not had the time to have an in-depth look of it.

\section{Evaluation method}
Our evaluation method, described above, consists in comparing the scores of the method implemented with the the \ac{LSB} ones.
They are computed by comparing the original watermarked data with the data recovered after decoding the altered signal. 
To alter the signal, we degrade it using a benchmarking algorithm. We implemented several algorithms based on the \brand{StirMark Benchmark for Audio}. They are described on figure \ref{frameworkclass2}.

\subsection{Algorithm resistance}
Once the algorithm were implemented, we used them to alter the watermarked signal so we could assess the strength of each method. Then we compute a score. 

The process is the following :
\begin{itemize}
 \item We watermark some data into an audio file.
 \item We alter it using a benchmark algorithm.
 \item We decode the signal and recover the watermarked data.
 \item We compare the original data with the recovered one : the bits that do not match are false bits.
\end{itemize}

\subsection{Auditory evaluation}
Another evaluation has also been done : the auditory evaluation. We made about ten persons listen to watermarked signals. 
We then took note of their opinion about the audio file they've listened to : if they heard something different, and if the audio quality was satisfactory. 

The results for both of these evaluations are present on chapter \ref{chap:results}.
